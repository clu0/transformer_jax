{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "from typing import Any, Callable, Sequence\n",
    "from jax import random, numpy as jnp\n",
    "import flax\n",
    "from flax import linen as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExplicitMLP(nn.Module):\n",
    "    features: Sequence[int]\n",
    "\n",
    "    def setup(self):\n",
    "        # we automatically know what to do with lists, dicts of submodules\n",
    "        self.layers = [nn.Dense(feat) for feat in self.features]\n",
    "        # for single submodules, we would just write:\n",
    "        # self.layer1 = nn.Dense(feat1)\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        x = inputs\n",
    "        for i, lyr in enumerate(self.layers):\n",
    "            x = lyr(x)\n",
    "            if i != len(self.layers) - 1:\n",
    "                x = nn.relu(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-13 21:36:02.765949: W external/xla/xla/service/gpu/nvptx_compiler.cc:718] The NVIDIA driver's CUDA version is 12.2 which is older than the ptxas CUDA version (12.4.131). Because the driver is older than the ptxas version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.\n"
     ]
    }
   ],
   "source": [
    "key1, key2 = random.split(random.key(0), 2)\n",
    "x = random.uniform(key1, (4,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialized parameter shapes:\n",
      " {'params': {'layers_0': {'bias': (3,), 'kernel': (4, 3)}, 'layers_1': {'bias': (4,), 'kernel': (3, 4)}, 'layers_2': {'bias': (5,), 'kernel': (4, 5)}}}\n"
     ]
    }
   ],
   "source": [
    "model = ExplicitMLP(features=[3,4,5])\n",
    "params = model.init(key2, x)\n",
    "\n",
    "print('initialized parameter shapes:\\n', jax.tree_util.tree_map(jnp.shape, flax.core.unfreeze(params)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output:\n",
      " [[ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.00723789 -0.00810346 -0.02550935  0.02151712 -0.01261239]\n",
      " [ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "y = model.apply(params, x)\n",
    "print('output:\\n', y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 4, 2, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape + (2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    n_embd: int\n",
    "    n_inner: int\n",
    "    p_dropout: float = 0.5\n",
    "    \n",
    "    def setup(self):\n",
    "        self.dense1 = nn.Dense(self.n_inner)\n",
    "        self.drop1 = nn.Dropout(rate=self.p_dropout)\n",
    "        self.dense2 = nn.Dense(self.n_embd)\n",
    "        self.drop2 = nn.Dropout(rate=self.p_dropout)\n",
    "        \n",
    "    def __call__(self, x, training: bool):\n",
    "        x = self.dense1(x)\n",
    "        x = nn.gelu(x)\n",
    "        x = self.drop1(x, deterministic=not training)\n",
    "        x = self.dense2(x)\n",
    "        x = self.drop2(x, deterministic=not training)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x, add_one: bool):\n",
    "    if add_one:\n",
    "        return x + 1\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "jit_f = jax.jit(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TracerBoolConversionError",
     "evalue": "Attempted boolean conversion of traced array with shape bool[]..\nThe error occurred while tracing the function f at /tmp/ipykernel_4386/3259198140.py:1 for jit. This concrete value was not available in Python because it depends on the value of the argument add_one.\nSee https://jax.readthedocs.io/en/latest/errors.html#jax.errors.TracerBoolConversionError",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTracerBoolConversionError\u001b[0m                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mjit_f\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "    \u001b[0;31m[... skipping hidden 11 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m, in \u001b[0;36mf\u001b[0;34m(x, add_one)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mf\u001b[39m(x, add_one: \u001b[38;5;28mbool\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m add_one:\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/jax/_src/core.py:1492\u001b[0m, in \u001b[0;36mconcretization_function_error.<locals>.error\u001b[0;34m(self, arg)\u001b[0m\n\u001b[1;32m   1491\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21merror\u001b[39m(\u001b[38;5;28mself\u001b[39m, arg):\n\u001b[0;32m-> 1492\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m TracerBoolConversionError(arg)\n",
      "\u001b[0;31mTracerBoolConversionError\u001b[0m: Attempted boolean conversion of traced array with shape bool[]..\nThe error occurred while tracing the function f at /tmp/ipykernel_4386/3259198140.py:1 for jit. This concrete value was not available in Python because it depends on the value of the argument add_one.\nSee https://jax.readthedocs.io/en/latest/errors.html#jax.errors.TracerBoolConversionError"
     ]
    }
   ],
   "source": [
    "jit_f(1, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer import Decoder\n",
    "from data import Dataloader\n",
    "import jax\n",
    "from jax import random\n",
    "import optax\n",
    "\n",
    "\n",
    "N_TRAIN = 10000\n",
    "LEARNING_RATE = 1e-5\n",
    "BLOCK_SIZE = 32\n",
    "BATCH_SIZE = 64\n",
    "N_LAYERS = 3\n",
    "N_EMBD = 256\n",
    "HEADS = 8\n",
    "N_INNER = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = Dataloader(batch_size=BATCH_SIZE, block_size=BLOCK_SIZE)\n",
    "decoder = Decoder(\n",
    "    n_layers=N_LAYERS,\n",
    "    n_vocab=data.n_vocab,\n",
    "    block_size=BLOCK_SIZE,\n",
    "    n_embd=N_EMBD,\n",
    "    heads=HEADS,\n",
    "    n_inner=N_INNER,\n",
    ")\n",
    "\n",
    "key1, key2, dropout_key = random.split(random.key(0), 3)\n",
    "x = random.randint(key1, (BATCH_SIZE, BLOCK_SIZE), minval=0, maxval=data.n_vocab)\n",
    "params = decoder.init(key2, x, training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4386/382811178.py:1: DeprecationWarning: jax.tree_map is deprecated: use jax.tree.map (jax v0.4.25 or newer) or jax.tree_util.tree_map (any JAX version).\n",
      "  jax.tree_map(lambda x: x.shape, params)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'params': {'blocks_0': {'attn': {'qkv': {'bias': (768,),\n",
       "     'kernel': (256, 768)}},\n",
       "   'ln1': {'bias': (256,), 'scale': (256,)},\n",
       "   'ln2': {'bias': (256,), 'scale': (256,)},\n",
       "   'mlp': {'dense1': {'bias': (512,), 'kernel': (256, 512)},\n",
       "    'dense2': {'bias': (256,), 'kernel': (512, 256)}}},\n",
       "  'blocks_1': {'attn': {'qkv': {'bias': (768,), 'kernel': (256, 768)}},\n",
       "   'ln1': {'bias': (256,), 'scale': (256,)},\n",
       "   'ln2': {'bias': (256,), 'scale': (256,)},\n",
       "   'mlp': {'dense1': {'bias': (512,), 'kernel': (256, 512)},\n",
       "    'dense2': {'bias': (256,), 'kernel': (512, 256)}}},\n",
       "  'blocks_2': {'attn': {'qkv': {'bias': (768,), 'kernel': (256, 768)}},\n",
       "   'ln1': {'bias': (256,), 'scale': (256,)},\n",
       "   'ln2': {'bias': (256,), 'scale': (256,)},\n",
       "   'mlp': {'dense1': {'bias': (512,), 'kernel': (256, 512)},\n",
       "    'dense2': {'bias': (256,), 'kernel': (512, 256)}}},\n",
       "  'final_ln': {'bias': (256,), 'scale': (256,)},\n",
       "  'logits': {'bias': (65,), 'kernel': (256, 65)},\n",
       "  'timestep_embd': {'embedding': (32, 256)},\n",
       "  'token_embd': {'embedding': (65, 256)}}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.tree_map(lambda x: x.shape, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = data.get_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[53, 56,  1, ..., 50, 43, 39],\n",
       "       [50,  1, 63, ..., 40, 43, 44],\n",
       "       [58, 53, 45, ..., 58,  1, 58],\n",
       "       ...,\n",
       "       [43, 58,  1, ..., 43,  1, 58],\n",
       "       [ 1, 39, 60, ..., 44,  1, 58],\n",
       "       [46, 43,  1, ..., 41, 53, 52]], dtype=int32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(nan, dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training loss fn\n",
    "logits = decoder.apply(params, x, training=True, rngs={\"dropout\": dropout_key})\n",
    "logits = logits.reshape(-1, data.n_vocab)\n",
    "y = y.reshape(-1, 1)\n",
    "optax.softmax_cross_entropy(logits, y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[56],\n",
       "       [ 1],\n",
       "       [46],\n",
       "       ...,\n",
       "       [53],\n",
       "       [52],\n",
       "       [42]], dtype=int32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[nan, nan, nan, ..., nan, nan, nan],\n",
       "       [nan, nan, nan, ..., nan, nan, nan],\n",
       "       [nan, nan, nan, ..., nan, nan, nan],\n",
       "       ...,\n",
       "       [nan, nan, nan, ..., nan, nan, nan],\n",
       "       [nan, nan, nan, ..., nan, nan, nan],\n",
       "       [nan, nan, nan, ..., nan, nan, nan]], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
